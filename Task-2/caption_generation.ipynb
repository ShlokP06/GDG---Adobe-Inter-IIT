{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOpWBiDFCOyU",
        "outputId": "04547a60-feaf-43cf-969e-db13849e5c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PyTorch: 2.8.0+cu126\n",
            "‚úÖ GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.41.0 torch pillow requests tqdm pandas \\\n",
        "  scikit-learn scikit-image accelerate bitsandbytes peft datasets \\\n",
        "  opencv-python imageio imageio-ffmpeg einops timm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2aMaz2ZCSsP",
        "outputId": "2570f7ff-fb7d-410d-e3f5-b58c854ea62b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Train data: 13864 samples\n",
            "‚úÖ Test data: 3467 samples\n",
            "\n",
            "Columns: ['id', 'date', 'likes', 'content', 'username', 'media', 'inferred company']\n",
            "\n",
            "Sample media:\n",
            "[Photo(previewUrl='https://pbs.twimg.com/media/D1LiAVeXQAAjpZw?format=png&name=small', fullUrl='https://pbs.twimg.com/media/D1LiAVeXQAAjpZw?format=png&name=large')]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive')\n",
        "DATA_CSV = \"/content/drive/MyDrive/adobe/train.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/adobe/output\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "df = pd.read_csv(DATA_CSV)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(f\"‚úÖ Train data: {len(train_df)} samples\")\n",
        "print(f\"‚úÖ Test data: {len(test_df)} samples\")\n",
        "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
        "print(f\"\\nSample media:\\n{train_df['media'].iloc[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtCLFy3uDTEz",
        "outputId": "ef81e02f-7544-4516-97ea-2d59ee92e3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flash_attn in /usr/local/lib/python3.12/dist-packages (2.8.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash_attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash_attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash_attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash_attn) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhWihfRzCWxS",
        "outputId": "75f24380-0be1-4690-890e-d96bc6e629ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "üöÄ LOADING FLORENCE-2 (GPU-OPTIMIZED)\n",
            "======================================================================\n",
            "\n",
            "Loading microsoft/Florence-2-large on cuda...\n",
            "‚úÖ Florence-2 and processor loaded\n",
            "GPU Memory: 1.55 GB\n",
            "‚úÖ GPU-optimized caption function ready\n",
            "\n",
            "üîç GPU Setup Check:\n",
            "  Model device: cuda:0\n",
            "  GPU allocated: 1.55GB\n",
            "  GPU reserved: 3.13GB\n",
            "  GPU total: 15.83GB\n",
            "\n",
            "  Testing inference on GPU...\n",
            "  ‚úÖ Inference time: 0.04s\n",
            "  ‚ö†Ô∏è No caption generated (might be test image)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ LOADING FLORENCE-2 (GPU-OPTIMIZED)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"microsoft/Florence-2-large\"\n",
        "\n",
        "print(f\"\\nLoading {model_id} on {device}...\")\n",
        "florence_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    trust_remote_code=True\n",
        ").to(device)\n",
        "florence_processor = AutoProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Florence-2 and processor loaded\")\n",
        "print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\" if device == \"cuda\" else \"\")\n",
        "def generate_caption_gpu_optimized(image, task=\"<MORE_DETAILED_CAPTION>\"):\n",
        "    \"\"\"\n",
        "    GPU-optimized caption generation\n",
        "    Keeps everything on GPU for speed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        inputs = florence_processor(\n",
        "            text=task,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        if device == \"cuda\":\n",
        "            for key in inputs:\n",
        "                if torch.is_tensor(inputs[key]):\n",
        "                    inputs[key] = inputs[key].to(device, torch.float16)\n",
        "        with torch.no_grad():\n",
        "            generated_ids = florence_model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                pixel_values=inputs[\"pixel_values\"],\n",
        "                max_new_tokens=256,\n",
        "                num_beams=3\n",
        "            )\n",
        "        result = florence_processor.batch_decode(\n",
        "            generated_ids,\n",
        "            skip_special_tokens=False\n",
        "        )[0]\n",
        "        parsed = florence_processor.post_process_generation(\n",
        "            result,\n",
        "            task=task,\n",
        "            image_size=(image.width, image.height)\n",
        "        )\n",
        "\n",
        "        return parsed.get(task, \"\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "print(\"‚úÖ GPU-optimized caption function ready\")\n",
        "def check_gpu_setup():\n",
        "    \"\"\"Verify everything is on GPU\"\"\"\n",
        "    print(\"\\nüîç GPU Setup Check:\")\n",
        "    try:\n",
        "        param = next(florence_model.parameters())\n",
        "        model_device = param.device\n",
        "        print(f\"  Model device: {model_device}\")\n",
        "    except:\n",
        "        print(\"  Model device: Unknown\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  GPU allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "        print(f\"  GPU reserved: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
        "        print(f\"  GPU total: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
        "    print(\"\\n  Testing inference on GPU...\")\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
        "\n",
        "    import time\n",
        "    start = time.time()\n",
        "    caption = generate_caption_gpu_optimized(test_img)\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    print(f\"  ‚úÖ Inference time: {elapsed:.2f}s\")\n",
        "    if caption:\n",
        "        print(f\"  ‚úÖ Generated: {caption[:50]}...\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è No caption generated (might be test image)\")\n",
        "\n",
        "check_gpu_setup()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5j3z0GKCek-",
        "outputId": "b6ee6d5b-db21-4119-fe05-e271f501ab2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions ready\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import cv2\n",
        "import imageio\n",
        "import requests\n",
        "import tempfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Tuple\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "def parse_media_string(media_str):\n",
        "    \"\"\"Parse Twitter media object\"\"\"\n",
        "    if not media_str or str(media_str) == 'nan':\n",
        "        return []\n",
        "    media_str = str(media_str)\n",
        "    media_list = []\n",
        "    try:\n",
        "        pattern = r'(Photo|Video|Gif)\\((.*?)\\)'\n",
        "        matches = re.findall(pattern, media_str, re.DOTALL)\n",
        "\n",
        "        for media_type, content in matches:\n",
        "            media_dict = {\n",
        "                \"type\": media_type.lower(),\n",
        "                \"thumbnail_url\": \"\",\n",
        "                \"video_url\": \"\",\n",
        "                \"content_type\": \"\",\n",
        "            }\n",
        "\n",
        "            thumb_match = re.search(r\"previewUrl='([^']+)'\", content) or \\\n",
        "                         re.search(r\"thumbnailUrl='([^']+)'\", content)\n",
        "            if thumb_match:\n",
        "                media_dict[\"thumbnail_url\"] = thumb_match.group(1)\n",
        "\n",
        "            video_match = re.search(r\"url='([^']+\\.mp4)'\", content)\n",
        "            if video_match:\n",
        "                media_dict[\"video_url\"] = video_match.group(1)\n",
        "                media_dict[\"content_type\"] = \"video/mp4\"\n",
        "\n",
        "            type_match = re.search(r\"contentType='([^']+)'\", content)\n",
        "            if type_match:\n",
        "                media_dict[\"content_type\"] = type_match.group(1)\n",
        "\n",
        "            media_list.append(media_dict)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return media_list\n",
        "\n",
        "def get_best_media_url(media_list):\n",
        "    \"\"\"Get best URL from media objects\"\"\"\n",
        "    for media in media_list:\n",
        "        if media.get('video_url'):\n",
        "            return (media['video_url'], media['type'], media.get('content_type', 'video/mp4'))\n",
        "        if media.get('thumbnail_url'):\n",
        "            return (media['thumbnail_url'], media['type'], 'image')\n",
        "    return (\"\", \"\", \"\")\n",
        "\n",
        "def detect_media_type(url, content_type, parsed_type):\n",
        "    \"\"\"Detect actual media type\"\"\"\n",
        "    url_lower = str(url).lower()\n",
        "    content_type_lower = str(content_type).lower()\n",
        "\n",
        "    if 'mp4' in content_type_lower or 'video' in content_type_lower:\n",
        "        return 'video'\n",
        "    if 'gif' in content_type_lower:\n",
        "        return 'gif'\n",
        "    if any(ext in url_lower for ext in ['.mp4']):\n",
        "        return 'video'\n",
        "    if any(ext in url_lower for ext in ['.gif']):\n",
        "        return 'gif'\n",
        "    if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):\n",
        "        return 'image'\n",
        "    if parsed_type == 'photo':\n",
        "        return 'image'\n",
        "    if parsed_type in ['video', 'gif']:\n",
        "        return parsed_type\n",
        "    return 'image'\n",
        "def fetch_media(url, timeout=10):\n",
        "    \"\"\"Download media\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        return BytesIO(response.content)\n",
        "    except:\n",
        "        return None\n",
        "def calculate_entropy(frame):\n",
        "    \"\"\"Calculate frame entropy\"\"\"\n",
        "    try:\n",
        "        if isinstance(frame, Image.Image):\n",
        "            frame = np.array(frame)\n",
        "\n",
        "        if len(frame.shape) == 3:\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = frame\n",
        "\n",
        "        hist, _ = np.histogram(gray.flatten(), bins=256, range=(0, 256))\n",
        "        hist = hist / (hist.sum() + 1e-7)\n",
        "        return scipy_entropy(hist)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def extract_frames_entropy(url, media_type, num_frames=5):\n",
        "    \"\"\"Extract frames using entropy-based selection\"\"\"\n",
        "    try:\n",
        "        media_file = fetch_media(url)\n",
        "        if media_file is None:\n",
        "            return []\n",
        "        if media_type == 'image':\n",
        "            img = Image.open(media_file).convert('RGB')\n",
        "            return [img]\n",
        "        elif media_type in ['video', 'gif']:\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp:\n",
        "                tmp.write(media_file.getvalue())\n",
        "                tmp_path = tmp.name\n",
        "\n",
        "            reader = imageio.get_reader(tmp_path)\n",
        "            total_frames = len(reader)\n",
        "\n",
        "            if total_frames == 0:\n",
        "                return []\n",
        "            sample_indices = np.linspace(0, total_frames - 1, min(15, total_frames), dtype=int)\n",
        "            sampled_frames = []\n",
        "            for idx in sample_indices:\n",
        "                try:\n",
        "                    frame = reader.get_data(idx)\n",
        "                    img = Image.fromarray(frame).convert('RGB')\n",
        "                    sampled_frames.append(img)\n",
        "                except:\n",
        "                    pass\n",
        "            if len(sampled_frames) == 0:\n",
        "                return []\n",
        "            entropies = [(calculate_entropy(np.array(img)), img) for img in sampled_frames]\n",
        "            entropies.sort(key=lambda x: x[0], reverse=True)\n",
        "            return [img for _, img in entropies[:num_frames]]\n",
        "        return []\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "print(\"‚úÖ Helper functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "01360fe574c74b539b176660e706cb2f",
            "283313b3cf2945809265874ebcacd935",
            "cc1006f1c295400f8df2789230c32c54",
            "43b7c4dd64e14834a03b657891869d17",
            "22ea869c04e64206bf679b69533bfae4",
            "e08448d2b78b44b791fc1c60055da5d9",
            "daf71e4e894e468b84313ea2bb5bad94",
            "62667535cc7a46a2bc9015eeaad3171c",
            "8a31e3ebcf594a80b2aae6360117ffc0",
            "e993a98bbc994d1e8994a5460596fae1",
            "9a2eaf54879345a8a2efda142071f65c"
          ]
        },
        "id": "fSPcdg1OC48c",
        "outputId": "9ea3576d-c802-40ce-db0b-f194f7cf17af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ TF32 enabled (faster on A100/RTX 30/40 series)\n",
            "‚úÖ Model compiled with torch.compile()\n",
            "\n",
            "üìä Creating optimized DataLoader...\n",
            "‚úÖ DataLoader created with:\n",
            "   - Batch size: 128\n",
            "   - Workers: 8 (async extraction)\n",
            "   - Prefetch: 4 batches\n",
            "   - Pin memory: Yes\n",
            "\n",
            "======================================================================\n",
            "‚ö° ULTRA-OPTIMIZED INFERENCE (Batch 128 + Async + Greedy Decode)\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01360fe574c74b539b176660e706cb2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Batches:   0%|          | 0/109 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _releaseLock at 0x7d3eb4989620>Exception ignored in: <function _releaseLock at 0x7d3eb4989620>\n",
            "\n",
            "Traceback (most recent call last):\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-956478893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing Batches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mflat_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                 \u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ResumeIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
            "Traceback (most recent call last):\n",
            "    KeyboardInterrupt"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import queue\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "print(\"‚úÖ TF32 enabled (faster on A100/RTX 30/40 series)\")\n",
        "try:\n",
        "    florence_model = torch.compile(florence_model, mode=\"reduce-overhead\")\n",
        "    print(\"‚úÖ Model compiled with torch.compile()\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è torch.compile not available\")\n",
        "\n",
        "class AsyncMediaDataset(Dataset):\n",
        "    \"\"\"Dataset with async frame extraction\"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, num_workers=8):\n",
        "        self.df = dataframe\n",
        "        self.num_workers = num_workers\n",
        "        self.frame_cache = {}\n",
        "        self.executor = ThreadPoolExecutor(max_workers=num_workers)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        media_str = str(row['media'])\n",
        "        try:\n",
        "            parsed = parse_media_string(media_str)\n",
        "            url, parsed_type, content_type = get_best_media_url(parsed)\n",
        "            actual_type = detect_media_type(url, content_type, parsed_type)\n",
        "            frames = extract_frames_entropy(url, actual_type, num_frames=5)\n",
        "        except:\n",
        "            frames = []\n",
        "\n",
        "        return {\n",
        "            'frames': frames,\n",
        "            'index': idx,\n",
        "            'media_type': actual_type if frames else 'none',\n",
        "            'id': row.get('id', idx)\n",
        "        }\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Efficient batching - flattens frames while tracking indices\"\"\"\n",
        "\n",
        "    all_frames = []\n",
        "    frame_to_sample = []\n",
        "    media_types = []\n",
        "    indices = []\n",
        "    ids = []\n",
        "\n",
        "    for sample_idx, sample in enumerate(batch):\n",
        "        frames = sample['frames']\n",
        "        all_frames.extend(frames)\n",
        "        for _ in frames:\n",
        "            frame_to_sample.append(sample_idx)\n",
        "\n",
        "        media_types.append(sample['media_type'])\n",
        "        indices.append(sample['index'])\n",
        "        ids.append(sample['id'])\n",
        "\n",
        "    return {\n",
        "        'frames': all_frames,\n",
        "        'frame_to_sample': frame_to_sample,\n",
        "        'media_types': media_types,\n",
        "        'indices': indices,\n",
        "        'ids': ids,\n",
        "        'batch_size': len(batch)\n",
        "    }\n",
        "print(\"\\nüìä Creating optimized DataLoader...\")\n",
        "media_dataset = AsyncMediaDataset(train_df, num_workers=8)\n",
        "data_loader = DataLoader(\n",
        "    media_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    prefetch_factor=4,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ DataLoader created with:\")\n",
        "print(f\"   - Batch size: 128\")\n",
        "print(f\"   - Workers: 8 (async extraction)\")\n",
        "print(f\"   - Prefetch: 4 batches\")\n",
        "print(f\"   - Pin memory: Yes\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚ö° ULTRA-OPTIMIZED INFERENCE (Batch 128 + Async + Greedy Decode)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = [None] * len(train_df)\n",
        "\n",
        "inf_batch_size = 32\n",
        "total_frames_processed = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Processing Batches\")):\n",
        "\n",
        "    flat_frames = batch['frames']\n",
        "    frame_to_sample = batch['frame_to_sample']\n",
        "    batch_size = batch['batch_size']\n",
        "\n",
        "    if not flat_frames:\n",
        "        continue\n",
        "    all_captions = []\n",
        "\n",
        "    for i in range(0, len(flat_frames), inf_batch_size):\n",
        "        sub_batch_frames = flat_frames[i:i+inf_batch_size]\n",
        "\n",
        "        try:\n",
        "            inputs = florence_processor(\n",
        "                text=\"<MORE_DETAILED_CAPTION>\",\n",
        "                images=sub_batch_frames,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            for key in inputs:\n",
        "                if torch.is_tensor(inputs[key]):\n",
        "                    inputs[key] = inputs[key].to(device, torch.float16)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated_ids = florence_model.generate(\n",
        "                    input_ids=inputs[\"input_ids\"],\n",
        "                    pixel_values=inputs[\"pixel_values\"],\n",
        "                    max_new_tokens=200,\n",
        "                    num_beams=1,\n",
        "                    do_sample=False,\n",
        "                    use_cache=True\n",
        "                )\n",
        "            captions_batch = florence_processor.batch_decode(\n",
        "                generated_ids,\n",
        "                skip_special_tokens=False\n",
        "            )\n",
        "            for caption_raw in captions_batch:\n",
        "                parsed = florence_processor.post_process_generation(\n",
        "                    caption_raw,\n",
        "                    task=\"<MORE_DETAILED_CAPTION>\",\n",
        "                    image_size=(224, 224)\n",
        "                )\n",
        "                caption = parsed.get(\"<MORE_DETAILED_CAPTION>\", \"\")\n",
        "                all_captions.append(caption)\n",
        "\n",
        "        except Exception as e:\n",
        "            all_captions.extend([\"\"] * len(sub_batch_frames))\n",
        "    sample_captions = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for frame_idx, caption in enumerate(all_captions):\n",
        "        sample_idx = frame_to_sample[frame_idx]\n",
        "        sample_captions[sample_idx].append(caption)\n",
        "\n",
        "    for local_idx, sample_idx in enumerate(batch['indices']):\n",
        "        combined_caption = \" \".join([c for c in sample_captions[local_idx] if c])\n",
        "\n",
        "        all_results[sample_idx] = {\n",
        "            'row_id': batch['ids'][local_idx],\n",
        "            'media_type': batch['media_types'][local_idx],\n",
        "            'combined_caption': combined_caption,\n",
        "            'num_frames': len(sample_captions[local_idx]),\n",
        "            'has_media': len(sample_captions[local_idx]) > 0\n",
        "        }\n",
        "\n",
        "    total_frames_processed += len(flat_frames)\n",
        "    if batch_idx % 5 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        fps = total_frames_processed / elapsed\n",
        "        eta_mins = (len(train_df) * 5 - total_frames_processed) / (fps * 5) / 60\n",
        "\n",
        "        print(f\"\\nüìä Progress: {batch_idx+1}/{len(data_loader)} batches\")\n",
        "        print(f\"   Frames/sec: {fps:.1f}\")\n",
        "        print(f\"   ETA: {eta_mins:.1f} mins\")\n",
        "\n",
        "elapsed_total = time.time() - start_time\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"‚úÖ INFERENCE COMPLETE\")\n",
        "print(f\"=\"*70)\n",
        "print(f\"Total time: {elapsed_total/60:.1f} minutes\")\n",
        "print(f\"Frames/sec: {total_frames_processed/elapsed_total:.1f}\")\n",
        "analysis_df = pd.DataFrame([r for r in all_results if r is not None])\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"  Processed: {len(analysis_df)}\")\n",
        "print(f\"  With captions: {(analysis_df['combined_caption'].str.len() > 0).sum()}\")\n",
        "print(f\"  Total frames: {analysis_df['num_frames'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1eBGTaVC6-m"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üíæ SAVING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_df_enhanced = train_df.copy()\n",
        "train_df_enhanced['florence_caption'] = analysis_df['combined_caption'].values\n",
        "train_df_enhanced['media_type'] = analysis_df['media_type'].values\n",
        "train_df_enhanced['num_frames'] = analysis_df['num_frames'].values\n",
        "train_df_enhanced['has_media'] = analysis_df['has_media'].values\n",
        "\n",
        "enhanced_csv = f\"{OUTPUT_DIR}/train_florence2_enhanced.csv\"\n",
        "train_df_enhanced.to_csv(enhanced_csv, index=False)\n",
        "\n",
        "print(f\"‚úÖ Saved: {enhanced_csv}\")\n",
        "print(f\"Shape: {train_df_enhanced.shape}\")\n",
        "\n",
        "sample = train_df_enhanced[train_df_enhanced['has_media'] == True].iloc[0]\n",
        "print(f\"\\nüìã Sample:\")\n",
        "print(f\"  Company: {sample['inferred company']}\")\n",
        "print(f\"  Media: {sample['media_type']}\")\n",
        "print(f\"  Frames: {sample['num_frames']}\")\n",
        "print(f\"  Caption: {sample['florence_caption'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A69kwopC78C"
      },
      "outputs": [],
      "source": [
        "print(\"Creating instructions...\")\n",
        "instructions = []\n",
        "responses = []\n",
        "for _, row in tqdm(train_df_enhanced.iterrows(), total=len(train_df_enhanced)):\n",
        "    company = str(row.get('inferred company', 'Unknown'))\n",
        "    username = str(row.get('username', company))\n",
        "    likes = int(row.get('likes', 0))\n",
        "    caption = str(row.get('florence_caption', ''))\n",
        "    content = str(row.get('content', ''))\n",
        "\n",
        "    context = f\"Media: {caption}\" if caption else \"No media\"\n",
        "\n",
        "    instruction = f\"\"\"Generate tweet for {company} (@{username}) targeting {likes} likes:\n",
        "{context}\n",
        "\n",
        "Tweet:\"\"\"\n",
        "\n",
        "    instructions.append(instruction)\n",
        "    responses.append(content)\n",
        "\n",
        "dataset_df = pd.DataFrame({'instruction': instructions, 'response': responses})\n",
        "instruction_csv = f\"{OUTPUT_DIR}/instructions_florence2.csv\"\n",
        "dataset_df.to_csv(instruction_csv, index=False)\n",
        "\n",
        "print(f\"‚úÖ {len(dataset_df)} instructions created\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 5024308,
          "sourceId": 45917,
          "sourceType": "competition"
        },
        {
          "datasetId": 2893282,
          "sourceId": 4988409,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2934068,
          "sourceId": 5053447,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2935195,
          "sourceId": 5055324,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2935198,
          "sourceId": 5055327,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2935266,
          "sourceId": 5055426,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2936493,
          "sourceId": 5057410,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4097887,
          "sourceId": 7107722,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4101452,
          "sourceId": 7117666,
          "sourceType": "datasetVersion"
        },
        {
          "sourceId": 120141306,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 120200988,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30397,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01360fe574c74b539b176660e706cb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_283313b3cf2945809265874ebcacd935",
              "IPY_MODEL_cc1006f1c295400f8df2789230c32c54",
              "IPY_MODEL_43b7c4dd64e14834a03b657891869d17"
            ],
            "layout": "IPY_MODEL_22ea869c04e64206bf679b69533bfae4"
          }
        },
        "22ea869c04e64206bf679b69533bfae4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283313b3cf2945809265874ebcacd935": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e08448d2b78b44b791fc1c60055da5d9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_daf71e4e894e468b84313ea2bb5bad94",
            "value": "Processing‚ÄáBatches:‚Äá‚Äá‚Äá0%"
          }
        },
        "43b7c4dd64e14834a03b657891869d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e993a98bbc994d1e8994a5460596fae1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9a2eaf54879345a8a2efda142071f65c",
            "value": "‚Äá0/109‚Äá[00:55&lt;?,‚Äá?it/s]"
          }
        },
        "62667535cc7a46a2bc9015eeaad3171c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a31e3ebcf594a80b2aae6360117ffc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a2eaf54879345a8a2efda142071f65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc1006f1c295400f8df2789230c32c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62667535cc7a46a2bc9015eeaad3171c",
            "max": 109,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a31e3ebcf594a80b2aae6360117ffc0",
            "value": 0
          }
        },
        "daf71e4e894e468b84313ea2bb5bad94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e08448d2b78b44b791fc1c60055da5d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e993a98bbc994d1e8994a5460596fae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
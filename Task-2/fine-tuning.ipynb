{"cells":[{"cell_type":"code","execution_count":null,"id":"38b5feb8-e1b9-4457-a501-b9b145d96ea8","metadata":{"id":"38b5feb8-e1b9-4457-a501-b9b145d96ea8","outputId":"72352255-ee4a-4338-a3c5-ec788c799d22"},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… GPU: NVIDIA A100-SXM4-40GB\n","âœ… VRAM: 42.4 GB\n"]}],"source":["!pip install -q transformers torch peft bitsandbytes datasets tqdm pandas\n","import torch\n","import pandas as pd\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n","\n","print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n","print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"]},{"cell_type":"code","execution_count":null,"id":"e3359fdf-9d52-4493-8934-bc179eb790cf","metadata":{"colab":{"referenced_widgets":["7dc76284a6ad4056b8c55e116e08f3d8"]},"id":"e3359fdf-9d52-4493-8934-bc179eb790cf","outputId":"fc254d98-cf03-43c1-eb33-28a68191e7fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["âœ… Data loaded: 13864 samples\n","Columns: ['instruction', 'response']\n","\n","Sample:\n","Instruction: Generate tweet for mcafee (@McAfee) targeting 2 likes:\n","No media\n","\n","Tweet:...\n","Response: To give our worldwide customers better #cybersecurity, we've extended our partne...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dc76284a6ad4056b8c55e116e08f3d8","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["DATA_CSV = \"adobe/output/instructions_optimized.csv\"\n","OUTPUT_DIR = \"lora\"\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","df = pd.read_csv(DATA_CSV)\n","\n","print(f\"âœ… Data loaded: {len(df)} samples\")\n","print(f\"Columns: {df.columns.tolist()}\")\n","print(f\"\\nSample:\")\n","print(f\"Instruction: {df['instruction'].iloc[0][:80]}...\")\n","print(f\"Response: {df['response'].iloc[0][:80]}...\")\n","from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"id":"bcd248ea-a54a-4c2a-8834-fa540e02e885","metadata":{"colab":{"referenced_widgets":["93a0efd2558a4ef993ece94936cbc823","f4cf6e737d37466b93bee5398c914413","1ae9e618b8644f92b78ae90d74aca9b4","41e3bc26d98f41de81f2526a810f8d3f","a9ba955cdc9c42e282ccdbed026a1989","0436c23cd10546c3a0f3761830a1d732","546263a587e84a55b7dd9a61c9ef7cf7","fa584e467da2448891404f2618242b40"]},"id":"bcd248ea-a54a-4c2a-8834-fa540e02e885","outputId":"6fb30b7c-5f87-4ef0-ea69-3364c51abbdb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93a0efd2558a4ef993ece94936cbc823","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4cf6e737d37466b93bee5398c914413","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ae9e618b8644f92b78ae90d74aca9b4","version_major":2,"version_minor":0},"text/plain":["Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41e3bc26d98f41de81f2526a810f8d3f","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9ba955cdc9c42e282ccdbed026a1989","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0436c23cd10546c3a0f3761830a1d732","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"546263a587e84a55b7dd9a61c9ef7cf7","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa584e467da2448891404f2618242b40","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ… Both loaded\n","âœ… Model loaded\n","GPU Memory: 4.45 GB\n"]}],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import BitsAndBytesConfig\n","import torch\n","model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n","model = AutoModelForCausalLM.from_pretrained(\n","    'mistralai/Mistral-7B-Instruct-v0.2',\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n",")\n","print(\"âœ… Both loaded\")\n","model.gradient_checkpointing_enable()\n","print(f\"âœ… Model loaded\")\n","print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b06175af-32c9-4624-a9af-267b3f199df7","metadata":{"id":"b06175af-32c9-4624-a9af-267b3f199df7","outputId":"4450bc49-15a7-4b95-c527-246764804956"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setting up LoRA...\n","trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n","âœ… LoRA applied with gradient checkpointing\n"]}],"source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","print(\"Setting up LoRA...\")\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.2,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",")\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","print(\"âœ… LoRA applied with gradient checkpointing\")"]},{"cell_type":"code","execution_count":null,"id":"e9611de9-d42c-41aa-9aca-1b78e1b22518","metadata":{"colab":{"referenced_widgets":["9efe305c1774409a82f2ed0a8b17084b"]},"id":"e9611de9-d42c-41aa-9aca-1b78e1b22518","outputId":"852c77ae-8042-4468-8c16-1a6ace7c8f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preparing dataset...\n","Dataset size: 13864\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9efe305c1774409a82f2ed0a8b17084b","version_major":2,"version_minor":0},"text/plain":["Tokenizing...:   0%|          | 0/13864 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ… Dataset prepared:\n","   Train: 12477\n","   Val: 1387\n","\n","Sample example keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n","input_ids shape: torch.Size([512])\n","labels shape: torch.Size([512])\n","âœ… Labels present on single example!\n"]}],"source":["from datasets import Dataset\n","print(\"Preparing dataset...\")\n","texts = []\n","for idx, row in df.iterrows():\n","    instruction = str(row['instruction'])\n","    response = str(row['response'])\n","    text = f\"[INST] {instruction} [/INST] {response}</s>\"\n","    texts.append(text)\n","raw_dataset = Dataset.from_dict({\"text\": texts})\n","print(f\"Dataset size: {len(raw_dataset)}\")\n","def tokenize_function(examples):\n","    tokenized = tokenizer(\n","        examples[\"text\"],\n","        max_length=512,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n","\n","    return tokenized\n","\n","# Apply tokenization\n","tokenized_dataset = raw_dataset.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=[\"text\"],\n","    desc=\"Tokenizing...\"\n",")\n","\n","# Split data\n","split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n","train_dataset = split_dataset[\"train\"]\n","eval_dataset = split_dataset[\"test\"]\n","\n","print(f\"âœ… Dataset prepared:\")\n","print(f\"   Train: {len(train_dataset)}\")\n","print(f\"   Val: {len(eval_dataset)}\")\n","\n","# Verify batch structure\n","sample_example = {k: torch.tensor(v[0]) for k, v in train_dataset[:1].items()}\n","\n","print(f\"\\nSample example keys: {sample_example.keys()}\")\n","print(f\"input_ids shape: {sample_example['input_ids'].shape}\")\n","print(f\"labels shape: {sample_example['labels'].shape}\")\n","print(f\"âœ… Labels present on single example!\")\n"]},{"cell_type":"code","execution_count":null,"id":"d9331f7a-52da-4275-ab89-19f1782f45f9","metadata":{"id":"d9331f7a-52da-4275-ab89-19f1782f45f9","outputId":"ebe927a9-7794-4cc3-caa9-bc2757bf7f73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setting up training arguments...\n","âœ… Training args ready\n"]}],"source":["from transformers import TrainingArguments\n","\n","print(\"Setting up training arguments...\")\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    num_train_epochs=2,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    gradient_accumulation_steps=2,\n","    learning_rate=2e-4,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.1,\n","    weight_decay=0.01,\n","    max_grad_norm=1.0,\n","    logging_steps=50,\n","    save_steps=200,\n","    eval_steps=200,\n","    save_total_limit=2,\n","    eval_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    fp16=True,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    report_to=[],\n",")\n","\n","print(\"âœ… Training args ready\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"24f87054-9cc6-401a-9864-91356d7d458e","metadata":{"id":"24f87054-9cc6-401a-9864-91356d7d458e","outputId":"82c3a920-b5ab-41a0-9073-197b39a69b3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating trainer...\n","âœ… Trainer created successfully\n","\n","Verifying batch format...\n","Batch keys: KeysView({'input_ids': tensor([[    2,     2,     2,  ...,  2403, 28767,     2],\n","        [    2,     2,     2,  ...,  2403, 28767,     2]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  2403, 28767,  -100],\n","        [ -100,  -100,  -100,  ...,  2403, 28767,  -100]])})\n","input_ids shape: torch.Size([2, 512])\n","labels shape: torch.Size([2, 512])\n","âœ… Batch format correct!\n"]}],"source":["from transformers import Trainer, DataCollatorForLanguageModeling\n","from torch.utils.data import DataLoader\n","print(\"Creating trainer...\")\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","train_loader = DataLoader(\n","    train_dataset, batch_size = 32, collate_fn = data_collator)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=data_collator,\n",")\n","\n","print(\"âœ… Trainer created successfully\")\n","print(\"\\nVerifying batch format...\")\n","batch = data_collator([train_dataset[i] for i in range(2)])\n","print(f\"Batch keys: {batch.keys()}\")\n","print(f\"input_ids shape: {batch['input_ids'].shape}\")\n","print(f\"labels shape: {batch['labels'].shape if 'labels' in batch else 'NOT PRESENT'}\")\n","print(f\"âœ… Batch format correct!\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b1dae35a-3666-4a1b-a565-29b4ea773d3c","metadata":{"id":"b1dae35a-3666-4a1b-a565-29b4ea773d3c","outputId":"4a9d91ad-1241-498b-ab83-a3294397cd1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["======================================================================\n","ðŸš€ STARTING TRAINING\n","======================================================================\n"]},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [780/780 1:21:51, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>1.436800</td>\n","      <td>1.439679</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.393000</td>\n","      <td>1.401593</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.322000</td>\n","      <td>1.386197</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","âœ… TRAINING COMPLETE\n","======================================================================\n","Training time: 1.37 hours\n"]}],"source":["import time\n","\n","print(\"=\"*70)\n","print(\"ðŸš€ STARTING TRAINING\")\n","print(\"=\"*70)\n","\n","start_time = time.time()\n","\n","trainer.train()\n","\n","elapsed = (time.time() - start_time) / 3600\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"âœ… TRAINING COMPLETE\")\n","print(\"=\"*70)\n","print(f\"Training time: {elapsed:.2f} hours\")\n"]},{"cell_type":"code","execution_count":null,"id":"4f6ff6bb-20b2-4a77-af82-6b8cde02a38a","metadata":{"id":"4f6ff6bb-20b2-4a77-af82-6b8cde02a38a","outputId":"b8791c9b-8bd3-4385-dd0b-cb224513d40e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model...\n","âœ… Model saved to: lora/final_model\n","   README.md (0.0 MB)\n","   chat_template.jinja (0.0 MB)\n","   tokenizer_config.json (0.0 MB)\n","   tokenizer.json (3.5 MB)\n","   special_tokens_map.json (0.0 MB)\n","   adapter_config.json (0.0 MB)\n","   adapter_model.safetensors (27.3 MB)\n"]}],"source":["print(\"Saving model...\")\n","final_model_dir = os.path.join(OUTPUT_DIR, \"final_model\")\n","os.makedirs(final_model_dir, exist_ok=True)\n","model.save_pretrained(final_model_dir)\n","tokenizer.save_pretrained(final_model_dir)\n","\n","print(f\"âœ… Model saved to: {final_model_dir}\")\n","import os\n","for file in os.listdir(final_model_dir):\n","    size = os.path.getsize(os.path.join(final_model_dir, file)) / 1e6\n","    print(f\"   {file} ({size:.1f} MB)\")\n"]},{"cell_type":"code","execution_count":null,"id":"2195987e-f60b-44a5-8812-c81e77737a49","metadata":{"colab":{"referenced_widgets":["077ec017f0ea44b7a5ff753e2c333816"]},"id":"2195987e-f60b-44a5-8812-c81e77737a49","outputId":"1a338a85-dbda-46ab-bac2-3d07a6a84c52"},"outputs":[{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"name":"stdout","output_type":"stream","text":["Loading fine-tuned model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"077ec017f0ea44b7a5ff753e2c333816","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["âœ… Fine-tuned model loaded\n"]}],"source":["from peft import AutoPeftModelForCausalLM\n","model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    model_path,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","print(\"âœ… Fine-tuned model loaded\")\n"]},{"cell_type":"code","execution_count":null,"id":"7c6f006e-7306-40fe-889a-4fba64a13d0d","metadata":{"id":"7c6f006e-7306-40fe-889a-4fba64a13d0d","outputId":"254ed2ec-a8e1-44de-d2d8-07cf57ccbb37"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating generation pipeline...\n","Generating predictions for 1387 samples...\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [06:15<00:00, 18.78s/it]"]},{"name":"stdout","output_type":"stream","text":["âœ… Generated 20 predictions\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","from transformers import pipeline\n","from transformers import logging as hf_logging\n","\n","hf_logging.set_verbosity_error()\n","\n","print(\"Creating generation pipeline...\")\n","\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=400,\n","    num_beams=1,\n","    temperature=0.7,\n",")\n","print(f\"Generating predictions for {len(eval_dataset)} samples...\")\n","\n","generated_text = []\n","\n","for i in tqdm(range(min(20, len(eval_dataset)))):\n","    sample = eval_dataset[i]\n","    prompt_ids = sample['input_ids'][:50]  # Take first 50 tokens as prompt\n","    prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n","\n","    try:\n","        result = pipe(prompt, num_return_sequences=1)\n","        generated = result[0]['generated_text'].split(prompt)[-1].strip()\n","        generated_text.append(generated)\n","    except Exception as e:\n","        generated_text.append(\"\")\n","\n","print(f\"âœ… Generated {len(generated_text)} predictions\")\n"]},{"cell_type":"code","execution_count":null,"id":"0fc6d1af-58e8-437f-b86f-12e7d146bb84","metadata":{"collapsed":true,"id":"0fc6d1af-58e8-437f-b86f-12e7d146bb84","outputId":"cde61d6b-1568-4247-96ee-e053f647dbf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results shape: (20, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>instruction</th>\n","      <th>reference</th>\n","      <th>generated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[INST] Generate tweet for emerson (@AnneTEmers...</td>\n","      <td>[INST] Generate tweet for emerson (@AnneTEmers...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[INST] Generate tweet for independent (@Indepe...</td>\n","      <td>[INST] Generate tweet for independent (@Indepe...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[INST] Generate tweet for aaa (@AAASouthPenn) ...</td>\n","      <td>[INST] Generate tweet for aaa (@AAASouthPenn) ...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[INST] Generate tweet for williams (@Rtreatwil...</td>\n","      <td>[INST] Generate tweet for williams (@Rtreatwil...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[INST] Generate tweet for hp (@HP) targeting 4...</td>\n","      <td>[INST] Generate tweet for hp (@HP) targeting 4...</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         instruction  \\\n","0  [INST] Generate tweet for emerson (@AnneTEmers...   \n","1  [INST] Generate tweet for independent (@Indepe...   \n","2  [INST] Generate tweet for aaa (@AAASouthPenn) ...   \n","3  [INST] Generate tweet for williams (@Rtreatwil...   \n","4  [INST] Generate tweet for hp (@HP) targeting 4...   \n","\n","                                           reference generated  \n","0  [INST] Generate tweet for emerson (@AnneTEmers...            \n","1  [INST] Generate tweet for independent (@Indepe...            \n","2  [INST] Generate tweet for aaa (@AAASouthPenn) ...            \n","3  [INST] Generate tweet for williams (@Rtreatwil...            \n","4  [INST] Generate tweet for hp (@HP) targeting 4...            "]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["instructions = []\n","references = []\n","\n","for i in range(min(20, len(eval_dataset))):\n","    sample = eval_dataset[i]\n","    instruction_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n","    reference_text = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n","    instructions.append(instruction_text)\n","    references.append(reference_text)\n","\n","results_df = pd.DataFrame({\n","    'instruction': instructions,\n","    'reference': references,\n","    'generated': generated_text,\n","})\n","\n","print(f\"Results shape: {results_df.shape}\")\n","results_df.head()"]},{"cell_type":"code","execution_count":null,"id":"a20ff4fc-fb02-4ba3-a8e0-69ec645e53e7","metadata":{"id":"a20ff4fc-fb02-4ba3-a8e0-69ec645e53e7","outputId":"52d5b5a1-a58b-43af-9838-ddb0cb1d48e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 20.02it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","ðŸ“ˆ PERPLEXITY\n","======================================================================\n","Mean Perplexity: 3.6349\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import math\n","model.eval()\n","perplexities = []\n","\n","with torch.no_grad():\n","    for text in tqdm(results_df['reference']):\n","        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n","        outputs = model(**inputs, labels=inputs['input_ids'])\n","        loss = outputs.loss\n","        perplexity = math.exp(loss.item())\n","        perplexities.append(perplexity)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ðŸ“ˆ PERPLEXITY\")\n","print(\"=\"*70)\n","print(f\"Mean Perplexity: {sum(perplexities)/len(perplexities):.4f}\")\n","\n","results_df['perplexity'] = perplexities\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}